# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
import argparse
import os
import sys
from pathlib import Path

import torch
import torch.backends.cudnn as cudnn

from utils.general import *
from models.experimental import attempt_load

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams, MyLoadImages
from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, time_sync

def init():
    cap = cv2.VideoCapture(0)
    return cap

def Histogram(frame):
    (b, g, r) = cv2.split(frame)  # é€šé“åˆ†ç¦»
    bH = cv2.equalizeHist(b)
    gH = cv2.equalizeHist(g)
    rH = cv2.equalizeHist(r)
    result = cv2.merge((bH, gH, rH))  # å®ç°å›¾åƒé€šé“çš„åˆå¹¶
    return result
# å¢åŠ è¿è¡Œå‚æ•°ï¼ŒåŸæ¥çš„å‚æ•°æ˜¯é€šè¿‡å‘½ä»¤è¡Œè§£æå¯¹è±¡æä¾›çš„ï¼Œè¿™é‡Œæ”¹ä¸ºç”±è°ƒç”¨è€…åœ¨ä»£ç ä¸­æä¾›ã€‚éœ€è¦ä¸€ä¸ª
# å¤§ä½“ä¸Šå®Œæˆä¸€æ ·åŠŸèƒ½çš„å‚æ•°å¯¹è±¡ã€‚
# æˆ‘æƒ³è¦çš„åŠŸèƒ½æ˜¯ä¼ ä¸€ç»„ç”±cv2è¯»å–çš„å›¾ç‰‡ï¼Œäº¤ç»™apiï¼Œç„¶åå¾—åˆ°ä¸€ç»„æ‰“ä¸Šæ ‡ç­¾çš„å›¾ç‰‡ï¼Œä»¥åŠæ¯å¼ å›¾ç‰‡å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«å¼•ç´¢ï¼Œä½ç½®ä¿¡æ¯ï¼Œç½®ä¿¡åº¦çš„ä¿¡æ¯ï¼Œè¿˜æœ‰ç±»åˆ«åç§°å­—å…¸
# è¦å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œéœ€è¦æƒé‡æ–‡ä»¶ï¼Œè¾“å…¥æ–‡ä»¶ä¸¤ä¸ªå‚æ•°ï¼Œå…¶ä»–å‚æ•°ä¸åŸä»£ç å‘½ä»¤è¡Œé»˜è®¤å‚æ•°ä¿æŒä¸€è‡´å°±è¡Œã€‚
class simulation_opt:# å‚æ•°å¯¹è±¡ã€‚

    def __init__(self,weights,img_size=640,conf_thres=0.25,iou_thres=0.45,device='',view_img=False,
                 classes=None,agnostic_nms=False,augment=False,update=False,exist_ok=False):
        self.weights=weights
        self.source=None
        self.img_size=img_size
        self.conf_thres=conf_thres
        self.iou_thres=iou_thres
        self.device=device
        self.view_img=view_img
        self.classes=classes
        self.agnostic_nms=agnostic_nms
        self.augment=augment
        self.update=update
        self.exist_ok=exist_ok

#å¢åŠ ä¸€ä¸ªæ–°ç±»ï¼Œè¿™ä¸ªæ–°ç±»æ˜¯åœ¨åŸæ¥detectå‡½æ•°ä¸Šè¿›è¡Œåˆ å‡ã€‚å¯ä»¥å…ˆå¤åˆ¶åŸæ¥çš„detectå‡½æ•°ä»£ç ï¼Œå†ç€æ‰‹ä¿®æ”¹
class detectapi:
    def __init__(self,weights,img_size=640):
        # æ„é€ å‡½æ•°ä¸­å…ˆåšå¥½å¿…è¦çš„å‡†å¤‡ï¼Œå¦‚åˆå§‹åŒ–å‚æ•°ï¼ŒåŠ è½½æ¨¡å‹
        #æ”¹ä¸º
        self.opt=simulation_opt(weights=weights,img_size=img_size)
        weights, imgsz= self.opt.weights, self.opt.img_size

    # Initialize
        set_logging()
        self.device = select_device(self.opt.device)
        self.half = self.device.type != 'cpu'  # half precision only supported on CUDA

    # Load model
        self.model = attempt_load(weights,self.device)  # load FP32 model
        self.stride = int(self.model.stride.max())  # model stride
        self.imgsz = check_img_size(imgsz, s=self.stride)  # check img_size
        if self.half:
            self.model.half()  # to FP16

    # Second-stage classifier
        self.classify = False
        if self.classify:
            self.modelc = load_classifier(name='resnet101', n=2)  # initialize
            self.modelc.load_state_dict(torch.load('weights/resnet101.pt',self.device)['model']).to(self.device).eval()
        '''
        self.names,å’Œself.colorsæ˜¯ç”±åé¢çš„ä»£ç æ‹‰åˆ°è¿™é‡Œæ¥çš„ã€‚namesæ˜¯ç±»åˆ«åç§°å­—å…¸ï¼Œcolorsæ˜¯ç”»æ¡†æ—¶ç”¨åˆ°çš„é¢œè‰²ã€‚
        '''
    # read names and colors
        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
        self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in self.names]


    def detect(self,source): # ä½¿ç”¨æ—¶ï¼Œè°ƒç”¨è¿™ä¸ªå‡½æ•°
        if type(source)!=list:
                raise TypeError('source must be a list which contain  pictures read by cv2')

        dataset = MyLoadImages(source, img_size=self.imgsz, stride=self.stride)
       

    # Run inference
        if self.device.type != 'cpu':
            self.model(torch.zeros(1, 3, self.imgsz, self.imgsz).to(self.device).type_as(next(self.model.parameters())))  # run once
        result=[]
        ''' åˆ æ‰
        for path, img, im0s, vid_cap in dataset: å› ä¸ºä¸ç”¨ä¿å­˜ï¼Œæ‰€ä»¥pathå¯ä»¥ä¸è¦ï¼Œå› ä¸ºä¸å¤„ç†è§†é¢‘ï¼Œæ‰€ä»¥vid_capä¸è¦ã€‚
        ''' #æ”¹ä¸º
        for img, im0s in dataset:
            img = torch.from_numpy(img).to(self.device)
            img = img.half() if self.half else img.float()  # uint8 to fp16/32
            img /= 255.0  # 0 - 255 to 0.0 - 1.0
            if img.ndimension() == 3:
                img = img.unsqueeze(0)

            # Inference
            # t1 = time_synchronized() #è®¡ç®—é¢„æµ‹ç”¨æ—¶çš„ï¼Œå¯ä»¥ä¸è¦
            pred = self.model(img, augment=self.opt.augment)[0]

            # Apply NMS
            pred = non_max_suppression(pred, self.opt.conf_thres, self.opt.iou_thres, classes=self.opt.classes, agnostic=self.opt.agnostic_nms)
            # t2 = time_synchronized() #è®¡ç®—é¢„æµ‹ç”¨æ—¶çš„ï¼Œå¯ä»¥ä¸è¦

            # Apply Classifier
            if self.classify:
                pred = apply_classifier(pred, self.modelc, img, im0s)
          # [tensor[[x1,y1,x2,y2,ç½®ä¿¡åº¦,ç±»åˆ«],[...],...]]
 # æ”¹ä¸º
            # Process detections
            det=pred[0] #åŸæ¥çš„æƒ…å†µæ˜¯è¦ä¿æŒå›¾ç‰‡ï¼Œå› æ­¤å¤šäº†å¾ˆå¤šå…³äºä¿æŒè·¯å¾„ä¸Šçš„å¤„ç†ã€‚å¦å¤–ï¼Œpred
            # å…¶å®æ˜¯ä¸ªåˆ—è¡¨ã€‚å…ƒç´ ä¸ªæ•°ä¸ºbatch_sizeã€‚ç”±äºå¯¹äºæˆ‘è¿™ä¸ªapiï¼Œæ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå›¾ç‰‡ï¼Œ
            # æ‰€ä»¥predä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œç›´æ¥å–å‡ºæ¥å°±è¡Œï¼Œä¸ç”¨forå¾ªç¯ã€‚
            im0 = im0s.copy() # è¿™æ˜¯åŸå›¾ç‰‡ï¼Œä¸è¢«ä¼ è¿›æ¥çš„å›¾ç‰‡æ˜¯åŒåœ°å€çš„ï¼Œéœ€è¦copyä¸€ä¸ªå‰¯æœ¬ï¼Œå¦åˆ™ï¼ŒåŸæ¥çš„å›¾ç‰‡ä¼šå—åˆ°å½±å“
            # s += '%gx%g ' % img.shape[2:]  # print string
            # gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            result_txt = []
            # å¯¹äºä¸€å¼ å›¾ç‰‡ï¼Œå¯èƒ½æœ‰å¤šä¸ªå¯è¢«æ£€æµ‹çš„ç›®æ ‡ã€‚æ‰€ä»¥ç»“æœæ ‡ç­¾ä¹Ÿå¯èƒ½æœ‰å¤šä¸ªã€‚
            # æ¯è¢«æ£€æµ‹å‡ºä¸€ä¸ªç‰©ä½“ï¼Œresult_txtçš„é•¿åº¦å°±åŠ ä¸€ã€‚result_txtä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸ªåˆ—è¡¨ï¼Œè®°å½•ç€
            # è¢«æ£€æµ‹ç‰©çš„ç±»åˆ«å¼•ç´¢ï¼Œåœ¨å›¾ç‰‡ä¸Šçš„ä½ç½®ï¼Œä»¥åŠç½®ä¿¡åº¦
            if len(det):
                # Rescale boxes from img_size to im0 size
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                # Print results
               
                # Write results

                for *xyxy, conf, cls in reversed(det):  # reversedä¸ºåå‘ç´¢å¼•
                    myclass = int(cls.item())
                    myconf = conf.item()

                    #xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                    #line = (int(cls.item()), [int(_.item()) for _ in xyxy], conf.item())  # label format
# item()è¿”å›çš„æ˜¯ä¸€ä¸ªæµ®ç‚¹å‹æ•°æ®
                    #result_txt.append(line)
                    #label = f'{self.names[int(cls)]} {conf:.2f}'
                    #annotator = Annotator(im0, line_width=3, example=None)
                    #annotator.box_label(xyxy, label, color = colors(cls))
            #result.append((im0,result_txt)) # å¯¹äºæ¯å¼ å›¾ç‰‡ï¼Œè¿”å›ç”»å®Œæ¡†çš„å›¾ç‰‡ï¼Œä»¥åŠè¯¥å›¾ç‰‡çš„æ ‡ç­¾åˆ—è¡¨ã€‚
       # return result, self.names
        return myclass,myconf



